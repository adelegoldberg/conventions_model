Generalizing from individuals to populations: Hierarchical inference supports convention formation

To communicate successfully, speakers and listeners must share a common system of semantic meaning in the language they are using. These meanings are conventional in the sense that they are sustained primarily by the expectations each agent has about others (Lewis, 1969). A key property of linguistic conventions is that they hold over an entire population, or community, allowing agents to communicate efficiently even with partners they've never met before. Accounts of how such population-level conventions come to be shared in the first place typically appeal to distributed local interactions, which may over time lead to emergent collective consensus (Baronchelli, 2018; Fay et al, 2010; Garrod & Doherty, 1994). But exactly how do agents make the inferential leap to population-level expectations from their individual experiences with specific partners? Is this generalization made in one step, with agents assuming that any conventional meanings established with one partner will automatically be shared with others? Or does it require interacting with multiple partners? In this study, we collected experimental data showing how people conventionalize referring expressions in a series of interactive reference games with different partners in a small community. Results were used to evaluate a hierarchical Bayesian cognitive model formalizing a theory of the generalization mechanisms underlying convention formation. 

Our proposal assumes that each agent has uncertainty over the system of meaning their current partner is using, and that they use Bayesian inference to update their beliefs about these meanings given observations of their partner’s language use and understanding. Critically, we also assume agents’ beliefs have hierarchical structure: the meanings used by different partners are expected to be sampled from a shared population-level distribution but may also differ from one another in meaningful ways. This structure provides an inductive pathway for abstract population-level expectations to be gradually distilled from partner-specific expectations. In other words, conventions result from solving a meta-learning problem.

The key empirical predictions distinguishing our model from alternatives concern behavior across partner boundaries. To test these predictions, 92 participants from Amazon Mechanical Turk were assigned to fully-connected four-person networks, for a total of 23 networks. Participants were then paired with each of their three neighbors for a series of dyadic interactions playing a real-time, natural-language reference game: one player (the speaker) provided descriptions target objects such that their partner (the listener) could choose it from an array of distractors. Each network was assigned four abstract tangram shapes taken from Clark & Wilkes-Gibbs (1986), and the trial sequence for a given partner was blocked so that each of the four objects appeared as the target four times. After completing all 16 trials with one partner, participants were introduced to a new partner and asked to play the same reference game again. This procedure was repeated until each participant had partnered with all three of their neighbors. 

We operationalized the degree of conventionalization as the mean number of words used per description, a standard measure of coding efficiency in reference games (Krauss & Weinheimer, 1964). While it has been frequently observed that messages reduce in length across repetitions with a single partner as the two partners establish local conventions through common ground, different accounts make different predictions at the partner boundary. Many previous agent-based models assume that agents do not distinguish one partner from another and completely transfer expectations from one partner to the next, which should result in no change in the number of words when a new partner is introduced. Another possibility is that there is no pooling across partners at all. This account predicts that agents must start from scratch establishing new partner-specific conventions with each new partner, resetting the initial description length anew with each interlocutor and never generalizing to the population-level. Contrary to either of these extremes, our hierarchical Bayesian model predicts that description length will increase at partner boundaries but that the initial length will decrease incrementally over successive interactions: after each partner, agents should be more willing to transfer expectations from one partner to another. 

We tested these predictions using mixed-effects regressions of partner number and repetition number on the number of words in a speaker's description, with random-effect structure including item-effects at the object and speaker level. We find a positive jump in description length across partner-boundaries overall, t(91) = 3.7, p < 0.001, indicating sensitivity to different partners, but a successive incremental decrease in the lengths of these initial descriptions, t(79.2) = -6.8, p < 0.001, consistent with our proposal. These results suggest that hierarchical generalization may be a foundational cognitive building block for establishing conventionality at the group level.
