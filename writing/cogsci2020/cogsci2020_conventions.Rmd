---
title: "Generalizing from individuals to populations: \\\ Hierarchical inference supports convention formation"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
   In this study, we collected experimental data showing how people conventionalize referring expressions in a series of interactive reference games with different partners in a small community. 
   Results were used to evaluate a hierarchical Bayesian cognitive model formalizing a theory of the generalization mechanisms underlying convention formation. 
    
keywords: >
    Add your choice of indexing terms or keywords; kindly use a semi-colon; between each term.
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(grid)
library(tidyverse)
library(tidyboot)
library(ggthemes)
library(xtable)

clicks <- read_csv('../../data/clicks.csv')
messages <- read_csv('../../data/messages.csv')

completeNetworks <- clicks %>% 
  distinct() %>% 
  group_by(networkid) %>% 
  tally() %>%
  filter(n == 96) %>% 
  pull(networkid)

numParticipantsRecruited <- length(unique(clicks$participantid))
numNetworks <- length(completeNetworks)

relevantMessages <- messages %>% 
  filter(networkid %in% completeNetworks) %>%
  filter(role == "speaker") %>%
  group_by_at(vars(-content)) %>%
  summarize(content = first(content)) %>%
  mutate(uttLength = str_count(content, " ") +1,
         repnum = floor(trialnum / 4)) %>%
  rowwise() %>%
  mutate(repnum = repnum + 1) %>%
  group_by(participantid, partnernum) %>%
  mutate(ordinalrep = ifelse(repnum == min(repnum), 'first', 'second')) %>%
  group_by(participantid, networkid, roomid, target, partnernum, repnum, ordinalrep) %>%
  summarize(m = sum(uttLength)) 
```

To communicate successfully, speakers and listeners must share a common system of semantic meaning in the language they are using. 
These meanings are *conventional* in the sense that they are sustained primarily by the expectations each agent has about others [@lewis_convention:_1969; @bicchieri_grammar_2006]. 
A key property of linguistic conventions is that they hold over entire communities allowing agents to communicate efficiently even with partners they've never met before. 
But exactly how do agents make the inferential leap to community-wide expectations from their experiences with specific partners? 
Grounding collective convention formation in individual cognitive mechanisms requires an explicit *theory of generalization* capturing how agents transfer what they have learned about one partner to new partners.

One influential theory is that speakers simply ignore the identity of different partners and update a single monolithic representation after every interaction [@steels_self-organizing_1995; @barr_establishing_2004; @young_evolution_2015; @baronchelli_emergence_2018]. 
We call this a *complete-pooling* theory [@gelman2006data] because data from each partner is collapsed into an undifferentiated pool of evidence. 
Complete-pooling models have been remarkably successful at predicting collective behavior on networks, but have typically been evaluated only in settings where anonymity is enforced. 
For example, @centola_spontaneous_2015 asked how large networks of participants coordinated on conventional names for novel faces.
On each trial, participants were paired with a random neighbors but were not informed of their partner's identity, or even the total number of different possible neighbors. 

While complete-pooling may be appropriate for some everyday social interactions, such as coordinating with anonymous drivers on the highway, it is less tenable for everyday communicative settings. 
Knowledge about a partner's identity is both available and relevant for conversation [@eckert_three_2012].
Extensive evidence from psycholinguistic studies has demonstrated the *partner-specificity* of our language use [@clark_using_1996;@brennan_partner-specific_2009; @horton_revisiting_2016]. 
Because meaning is grounded in the evolving `common ground' shared with each partner, meanings established over a history of interaction with one partner are not necessary transfered to other partners [@wilkes-gibbs_coordinating_1992; @metzing_when_2003]. 
Partner-specificity poses clear problems for complete-pooling theories but can be easily explained by a simple *no-pooling* model where agents maintain distinct representations of meaning for each partner. 
The problem with a no-pooling model, of course, is that it forces agents to start from scratch with each partner.
Community-level expectations can never get off the ground. 

What theory of generalization, then, can explain partner-specific meaning but also allow conventions to spread through communities? 
We propose a *partial-pooling* account that offers a compromise between these extremes. 
Unlike complete-pooling and no-pooling models, we propose that beliefs about meaning have hierarchical structure. 
That is, the meanings used by different partners are expected to be drawn from a shared community-wide distribution but are also allowed to differ from one another in systematic, partner-specific ways. 
This structure provides an inductive pathway for abstract population-level expectations to be distilled from partner-specific experience [see also @KleinschmidtJaeger15_RobustSpeechPerception; @tenenbaum_how_2011 for hierarchical accounts of abstraction in other domains]. 
In other words, we suggest that conventional meanings result from agents solving a meta-learning problem, adapting to each partner along the way.

We begin by formalizing this account in a probabilistic model of communication. 
We present simulations showing the pattern of listener and speaker behavior within and across partners. 
\todo[inline]{Add description of experiment and results}

# Hierarchical Bayesian model of generalization

```{r model_schematic, fig.env = "figure", fig.pos = "t", out.width = "225px", fig.height=1, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "Schematic of hierachical Bayesian model."}
knitr::include_graphics("figs/task1_model.pdf")
```

In this section, we provide an explicit computational account of the cognitive mechanisms driving this shift.
Specifically, we extend the dyadic convention formation model of @hawkins_convention-formation_2017 with a mechanism for generalization.
This model begins with the idea that knowledge about meanings can be represented probabilistically: agents have *lexical uncertainty* over what meaning their partner is using [@]. 
At the highest level is a \emph{task-general} variable $\Theta$ which parameterizes the agent's task-specific prior expectations $P(\theta_{i} | \Theta)$ where $\theta_i$ represents the semantics used by a novel partner $i$. 
Given observations $D_i$ from communicative interactions in that context, an agent can update their \emph{task-specific} model using Bayes rule:
\begin{equation}
	P(\theta_i | D_i, \Theta)  \propto P(D_i | \theta_i) P(\theta_i | \Theta)
\end{equation}
The Bayesian formulation thus decomposes the problem of task-specific adaptation into two terms, a prior term $P(\theta_i | \Theta)$ and a likelihood term$P(D_i | \theta_i)$.
The prior captures the idea that different language tasks share some task-general structure in common: in the absence of strong information about usage departing from this common structure, the agent ought to be regularized toward their task-general knowledge.

Our proposal assumes that each agent has uncertainty over the system of meaning their current partner is using, and that they use Bayesian inference to update their beliefs about these meanings given observations of their partnerâ€™s language use and understanding.
The key predictions distinguishing our model concern the pattern of generalization across partners.
First, we show that our model accounts for the \emph{partner-specificity} of ad hoc conventions as a consequence of hierarchical structure. 
Under our model, speakers revert back to a longer description with a novel partner because evidence from a single listener is relatively uninformative about the community-level prior.

This hierarchical structure, however, leads to a further prediction: after interacting with enough partners in a tight-knit community, speakers should become increasingly confident that labels are not simply idiosyncratic features of a particular partner's lexicon but are shared across the entire community.
In other words, the partner-specific expectations agents form within an interaction to solve a novel communication problem should gradually generalize to community-wide expectations as they gain additional evidence of the latent population-level distribution from which different partners are sampled.
These expectations should manifest in an increasing willingness to use short labels with novel partners, leading to the emergent consequence of lending additional evidence for that structure. 


```{r model_results, fig.env = "figure*", fig.pos = "t", fig.width=8, fig.height=4, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "Listener model expectations across a series of different partners."}
library(grid)
library(gridExtra)

model.plt1 <- read_csv('../../simulations/hierarchical/hierarchicalSimulationOutput.csv') %>%
  ggplot(aes(x = time, y = prediction, color = factor(partnerID), group = factor(partnerID))) +
    geom_point() +
    geom_line() +
    geom_hline(yintercept = .5) +
    geom_hline(yintercept = .5) +
    theme_few() +
    xlab('time') +
    ylab("P(target)") +
    scale_x_continuous(breaks = c(1,4,7,10)) +
    theme(aspect.ratio = 1)

model.plt2 <- read_csv('../../simulations/hierarchical/speakerOutput.csv') %>%
  ggplot(aes(x = time, y = exp(prediction), color = factor(partnerID), group = factor(partnerID))) +
    geom_point() +
    geom_line() +
    theme_few() +
    xlab('time') +
    ylab("P(long utterance)") +
    scale_x_continuous(breaks = c(1,4,7,10)) +
    theme(aspect.ratio = 1)

grid.arrange(model.plt1, model.plt2, ncol = 2)
```

# Experiment: Conventions on a network

```{r task_display, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "\\label{fig:task1_display} Experimental design. (A) Participants were placed in fully-connected networks of 4 and (B) played repeated reference games with each partner."}
knitr::include_graphics("figs/design.pdf")
```

To evaluate these qualitative predictions, we designed a communication experiment on a small network.
Rather than anonymizing partners, however, we divided the experiment into blocks of contiguous interaction with stable partners [see @fay_interactive_2010; @garrod_conversation_1994 for similar designs].
Each  blocks was a full repeated reference game, where participants had to coordinate on an ad hoc convention, or *pact*, for how to refer to reoccuring target objects with their partner.
While it has been frequently observed that messages reduce in length across repetitions as common ground is built a single partner [@krauss_changes_1964], and sharply jump revert to their initial length when a new partner is introduced [@wilkes-gibbs_coordinating_1992,@weber_cultural_2003], we are interested in the effect at subsequent partner boundaries. 
Complete-pooling accounts predict no change in the number of words when a new partner is introduced.  
No-pooling accounts predict that the same increase in initial description length will occur with every interlocutor. 
Contrary to either of these extremes, our hierarchical Bayesian model predicts that description length will increase at partner boundaries but that the initial length will decrease incrementally over successive interactions: after each partner, agents should be more willing to transfer expectations from one partner to another in their community. 

## Methods

### Participants 

We recruited  participants from Amazon Mechanical Turk to play an interactive, natural-language reference game implemented with the Dallinger platform\footnote{http://docs.dallinger.io/}.
Participants were randomly assigned to one of `r numNetworks` fully-connected four-person communities. 

### Stimuli and procedure

<!-- One player (the speaker) provided descriptions target objects such that their partner (the listener) could choose it from an array of distractors.  -->
<!-- Each network was assigned four abstract tangram shapes taken from Clark & Wilkes-Gibbs (1986), and the trial sequence for a given partner was blocked so that each of the four objects appeared as the target four times.  -->
<!-- After completing all 16 trials with one partner, participants were introduced to a new partner and asked to play the same reference game again.  -->
<!-- This procedure was repeated until each participant had partnered with all three of their neighbors.  -->

Participants were paired with each of their three neighbors for a series of dyadic interactions. 
In each interaction, they played a real-time, natural-language reference game where they repeatedly referred to a set of four abstract tangram shapes taken from [@clark_referring_1986; see Fig. \ref{fig:task1_display}].
These stimuli have been used extensively in the literature on coordination and common ground.
They were designed such that participants will not already have strong pre-existing lexical conventions for how to refer to them (unlike photographs of common objects), but are structured enough to support many possible descriptions (unlike images of white noise).

On each trial of a reference game, one of these four shapes was highlighted as the \emph{target object} for the "speaker" who was instructed to use a chatbox to communicate the identity of this object to their partner, the "listener".
The listener could reply through the chatbox but must ultimately make a selection from the array. 
The trial sequence for a given partner was constructed so that each of four targets appear six times each, spread evenly across the session, for a total of 24 trials.

After completing 24 trials with one partner, they were introduced to their next partner and asked to play the repeated reference game again with the same four objects.
Each participant in a network was assigned a distinct avatar so that participants were clear they were speaking to distinct partners.
This process repeated until each participant had partnered with all five neighbors.
Players were given full feedback on each about their partner's choice and received bonus payment for each correct response. 
Because some pairs within the network took longer than others to complete the trial sequence, we sent participants to a temporary waiting room if their next partner was not yet ready. 


## Results

### Utterance length

```{r reduction, cache=T, fig.env = "figure", fig.pos = "h", fig.width=3, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "Reduction in number of words within and across partner boundaries."}
relevantMessages %>%
  mutate(partnerlabel = paste0(c('partner #', partnernum + 1), collapse = "")) %>%
  group_by(partnerlabel, repnum) %>% 
  tidyboot::tidyboot_mean(column = m) %>%
  ggplot(aes(x = repnum, y = empirical_stat)) +
    geom_line() +
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = .2) +
    facet_wrap(~ partnerlabel) +
    theme_few() +
    ylim(0,NA) +
    ylab("mean # words") +
    xlab("repetition with partner")
```

The key empirical predictions distinguishing our model from alternatives concern behavior across partner boundaries.
We operationalized the degree of conventionalization as the mean number of words used per description, a standard measure of coding efficiency in reference games . 
We tested these predictions using mixed-effects regressions of partner number and repetition number on the number of words in a speaker's description, with random-effect structure including item-effects at the object and speaker level. We find a positive jump in description length across partner-boundaries overall, t(91) = 3.7, p < 0.001, indicating sensitivity to different partners, but a successive incremental decrease in the lengths of these initial descriptions, t(79.2) = -6.8, p < 0.001, consistent with our proposal. These results suggest that hierarchical generalization may be a foundational cognitive building block for establishing conventionality at the group level.

### Convergence 

# Discussion

1. Other advantages of hierarchical model. e.g. it's more robust to deviations than complete-pooling; if we have a lot of interactions with idiosyncratic speakers (e.g. children), we don't replace our conventional community-level expectations. But agent-based models with a memory window or single representation predict this. Also, 

2. Possible connection to memory mechanisms: partners as contexts that get reinstated [@Brown-schmidt], and 

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
