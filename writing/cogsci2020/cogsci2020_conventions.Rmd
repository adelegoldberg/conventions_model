---
title: |
    | Generalizing from individuals to populations:
    | Hierarchical inference supports convention formation on networks
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
   Linguistic conventions allow us to communicate efficiently even with novel members of our community.
   At the same time, much of our intended meaning is partner-specific.  
   Exactly how do agents make the inferential leap to community-wide expectations from their experiences with specific partners?
   We propose a hierarchical Bayesian model that explains how partner-specific pacts may spread through a network and present simulations of how speakers and listeners generalize meanings across partners. 
   To evalute these predictions, we collected experimental data showing how people conventionalize referring expressions in a series of interactive reference games with different partners in a small community. 
   These results suggest that local partner-specific learning is not only compatible with global convention formation but may facilitate it when coupled with a powerful hierachical inductive mechanism. 

    
keywords: >
   convention; generalization; 
    
output: cogsci2016::cogsci_paper 
#final-submission: \cogscifinalcopy
---


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(grid)
library(tidyverse)
library(tidyboot)
library(ggthemes)
library(xtable)

clicks <- read_csv('../../data/clicks.csv')
messages <- read_csv('../../data/messages.csv')

completeNetworks <- clicks %>% 
  distinct() %>% 
  group_by(networkid) %>% 
  tally() %>%
  filter(n == 96) %>% 
  pull(networkid)

numParticipantsRecruited <- length(unique(clicks$participantid))
numNetworks <- length(completeNetworks)

relevantMessages <- messages %>% 
  filter(networkid %in% completeNetworks) %>%
  filter(role == "speaker") %>%
  group_by_at(vars(-content)) %>%
  summarize(content = first(content)) %>%
  mutate(uttLength = str_count(content, " ") +1,
         repnum = floor(trialnum / 4)) %>%
  rowwise() %>%
  mutate(repnum = repnum + 1) %>%
  group_by(participantid, partnernum) %>%
  mutate(ordinalrep = ifelse(repnum == min(repnum), 'first', 'second')) %>%
  group_by(participantid, networkid, roomid, target, partnernum, repnum, ordinalrep) %>%
  summarize(m = sum(uttLength)) 

relevantClicks <- clicks %>%
  filter(networkid %in% completeNetworks) %>%
  mutate(repnum = floor(trialnum / 4)) %>%
  mutate(correct = object_id == 'target') 
```

To communicate successfully, speakers and listeners must share a common system of semantic meaning in the language they are using. 
These meanings are *conventional* in the sense that they are sustained by the expectations each person has about others [@lewis_convention:_1969; @bicchieri_grammar_2006]. 
A key property of linguistic conventions is that they hold over an entire community of speakers, allowing us to communicate efficiently even with people we've never met before. 
But exactly how do we make the inferential leap to community-wide expectations from our experiences with specific partners? 
Grounding collective convention formation in individual cognition requires an explicit *theory of generalization* capturing how people transfer what they have learned from one partner to the next.

One influential theory is that speakers simply ignore the identity of different partners and update a single monolithic representation after every interaction [@steels_self-organizing_1995; @barr_establishing_2004; @young_evolution_2015; @baronchelli_emergence_2018]. 
We call this a *complete-pooling* theory because data from each partner is collapsed into an undifferentiated pool of evidence [@gelman2006data]. 
Complete-pooling models have been remarkably successful at predicting collective behavior on networks, but have typically been evaluated only in settings where anonymity is enforced. 
For example, @centola_spontaneous_2015 asked how large networks of participants coordinated on conventional names for novel faces.
On each trial, participants were paired with a random neighbor but were not informed of that neighbor's identity, or even the total number of different possible neighbors. 

While complete-pooling may be appropriate for some everyday social interactions, such as coordinating with anonymous drivers on the highway, it is less tenable for everyday communicative settings. 
Knowledge about a partner's identity is both available and relevant for conversation [@eckert_three_2012].
Extensive evidence from psycholinguistics has demonstrated the *partner-specificity* of our language use [@clark_using_1996;@brennan_partner-specific_2009]. 
Because meaning is grounded in the evolving 'common ground' shared with each partner, meanings established over a history of interaction with one partner are not necessary transfered to other partners [@wilkes-gibbs_coordinating_1992; @metzing_when_2003]. 
Partner-specificity thus poses clear problems for complete-pooling theories but can be easily explained by another simple model, where agents maintain separate expectations about meaning for each partner. 
We call this a *no-pooling* model. 
The problem with no-pooling, of course, is that agents are forced to start from scratch with each partner.
Community-level expectations never get off the ground. 

What theory of generalization, then, can explain partner-specific meaning but also allow conventions to spread through communities? 
We propose a *partial-pooling* account that offers a compromise between these extremes. 
Unlike complete-pooling and no-pooling models, we propose that beliefs about meaning have hierarchical structure. 
That is, the meanings used by different partners are expected to be drawn from a shared community-wide distribution but are also allowed to differ from one another in systematic, partner-specific ways. 
This structure provides an inductive pathway for abstract population-level expectations to be distilled from partner-specific experience [see also @KleinschmidtJaeger15_RobustSpeechPerception; @tenenbaum_how_2011]. 


We begin by formalizing this account in a probabilistic model of communication and presenting several simulations of listener and speaker behavior within and across partners. 
Next, we test the qualitative predictions of this model in a behavioral experiment.
Participants were paired for a series of extended reference games with each neighbor in small networks. 
Our results showed signatures of *ad hoc* convention formation within dyads, but also gradual generalization of these local pacts across subsequent partners as the network converged. 
Taken together, these results suggest that local partner-specific learning is not only compatible with global convention formation but may facilitate it when coupled with a powerful hierachical inductive mechanism. 

# A hierarchical Bayesian model of convention

```{r model_schematic, fig.env = "figure", fig.pos = "t", out.width = "225px", fig.height=1, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:task1model} Schematic of hierachical Bayesian model."}
knitr::include_graphics("figs/task1_model.pdf")
```

In this section, we provide an explicit computational account of the cognitive mechanisms supporting the balance between community-level stability and partner-specific flexibility.
Specifically, we show how the dyadic convention formation model of @hawkins_convention-formation_2017 can be extended with a principled mechanism for generalization across multiple partners.
This model begins with the idea that knowledge about meanings can be represented probabilistically: agents have uncertainty about what lexical meaning their current partner is using [@bergen_pragmatic_2016]. 
In our hierarchical model, this lexical uncertainty is represented by a multi-level prior. 

At the highest level of the hierarchy is a *community-level* variable $\Theta$ parameterizing the agent's *partner-specific* expectations $P(\phi_{k} | \Theta)$, where $\phi_k$ represents the latent system of meanings used by partner $k$ (see Fig. \ref{fig:task1model}). 
Given observations $D_k$ from repeated communicative interactions with a specific partner $i$, the agent updates their beliefs about the latent system of meaning using Bayes rule:
\begin{equation}
\begin{array}{rcl}
P(\phi_k, \Theta | D_k)  & \propto &  P(D_k | \phi_k, \Theta) P(\phi_k, \Theta) \\
                           & =   & P(D_k | \phi_k) P(\phi_k | \Theta) P(\Theta)
\end{array}
\end{equation}

This inference decomposes the problem of partner-specific learning into two terms, a prior term $P(\phi_k | \Theta)P(\Theta)$ and a likelihood term $P(D_k | \phi_k)$.
The prior captures the idea that different partners will share some aspects of meaning in common.
In the absence of strong information about partner-specific language use departing from this common structure, the agent ought to be regularized toward generalizable knowledge of their community's conventions [@davidson_nice_1986].
The likelihood represents predictions about how a partner using a particular system of meaning will use language.

This joint posterior over meanings has two consequences for convention formation.
First, it allows agents to maintain partner-specific expectations $\phi_k$ by marginalizing over community-level uncertainty:
\begin{equation}
P(\phi_k | D_k) = \int_{\Theta}P(D_k | \phi_k) P(\phi_k | \Theta) P(\Theta)  d\Theta
\end{equation}
Second, the hierarchical structure provides an inductive pathway for data to inform beliefs about community-wide conventions.
Agents update their beliefs about $\Theta$ by marginalizing over data accumulated from different partners:
\begin{equation}
P(\Theta | D) = P(\Theta) \int_{\phi} P(D_k | \phi_k) P(\phi_k | \Theta) d\phi
\end{equation}
where $D = \bigcup_{k=1}^N D_k$, $\phi = \phi_1 \times \dots \times \phi_N$, and $N$ is the number of partners previous encountered. 
Given weak community-level expectations about $\Theta$, a particular partner's behavior may at first be more parsimoniously explained with a partner-specific model.
After multiple partners are inferred to have a similar system of meaning, however, beliefs about $\Theta$ shift to represent this abstracted knowledge: it becomes more likely that a novel partner will share it as well.
This transfer is sometimes referred to as a "sharing of strength" or "partial pooling" [@gelman2006data] because abstractions are smoothly integrated with domain-specific detail depending on the data available.

## Model simulations

```{r cache=T}
unfinished <- read_csv('../../simulations/hierarchical/compiled_network_output.csv') %>%
  group_by(chainNum) %>%
  tally() %>%
  filter(n < 24) %>%
  pull(chainNum)

modelNetworks <- read_csv('../../simulations/hierarchical/compiled_network_output.csv') %>%
  filter(!(chainNum %in% unfinished)) %>%
  mutate(row_number = row_number() - 1,
       chainNum = floor(row_number / 24),
       repNum = row_number %% 4,
       correct = objectPicked == 'object1') 

modelconvergence.toPlot <- modelNetworks %>%
  filter(!(chainNum %in% unfinished)) %>%
  group_by(chainNum, speakerID, partnerID) %>%
  mutate(ordinalRep = ifelse(time == first(time), 'first', 'second')) %>%
  select(ordinalRep, speakerID,  partnerID, utt, chainNum) %>%
  group_by(chainNum, ordinalRep, partnerID) %>%
  tidybayes::gather_pairs(speakerID, utt, row = 'speaker1', col = 'speaker2', x = 'utt1', y = 'utt2') %>%
  mutate(partnerType = case_when(partnerID == 1 ~ ifelse((speaker1 == 2 && speaker2 == 1) ||
                                                            (speaker1 == 4 && speaker2 == 3),
                                                          'within', 'between'),
                                 partnerID == 2 ~ ifelse((speaker1 == 3 && speaker2 == 1) ||
                                                            (speaker1 == 4 && speaker2 == 2),
                                                          'within', 'between'),
                                 partnerID == 3 ~ ifelse((speaker1 ==4 && speaker2 == 1) ||
                                                            (speaker1 == 3 && speaker2 == 2),
                                                          'within', 'between')),
         match = utt1 == utt2) %>%
  filter(utt1 %in% c('word1', 'word2') && utt2 %in% c('word1', 'word2')) %>%
  group_by( partnerID, partnerType) %>%
  tidyboot_mean(match) 
```

```{r model_results, cache = T, fig.env = "figure*", fig.pos = "t!", fig.width=7, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:simulations} Model predictions across a series of different partners."}
library(grid)
library(gridExtra)

model.plt1 <- read_csv('../../simulations/hierarchical/listenerOutput.csv') %>%
  ggplot(aes(x = time, y = prediction, color = factor(partnerID), group = factor(partnerID))) +
    geom_point() +
    geom_line() +
    geom_hline(yintercept = .5) +
    theme_few() +
    xlab('time') +
    ylab("accuracy") +
    scale_x_continuous(breaks = c(1,3, 5,7,9,11,13,15)) +
    scale_color_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    guides(color = FALSE) +
    labs(tag = "A", title = "Listener") +
    theme(aspect.ratio = 4/5)

model.plt2 = read_csv('../../simulations/hierarchical/speakerOutput_reps.csv') %>%
  mutate(expectedNumWords = 2 * exp(prediction) + 1 * (1 - exp(prediction))) %>%
  group_by(time, partnerID) %>%
  tidyboot_mean(expectedNumWords) %>%
  ggplot(aes(x = time, y = empirical_stat, fill = factor(partnerID), color = factor(partnerID), group = factor(partnerID))) +
    geom_point() +
    geom_line() +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.3, color=NA) +
    theme_few() +
    ylim(1, 2) +
    geom_hline(yintercept = 1) +
    xlab('time') +
    ylab("# words") +
    scale_x_continuous(breaks = c(1,3, 5,7,9,11,13,15)) +
    scale_color_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    scale_fill_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    guides(color = FALSE, fill = FALSE) +
    labs(tag = "B", title = "Speaker") +
    theme(aspect.ratio = 4/5)

model.plt3 <- modelconvergence.toPlot %>%
  ungroup() %>%
  mutate(comparison = ifelse(partnerType == 'within', 'within dyad', 'across dyads')) %>%
  ggplot(aes(x = partnerID, y = empirical_stat, color = comparison, group = comparison)) +
    geom_point() +
    geom_line() +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    labs(tag = "C", title = "Network") +
    theme_few() +
    ylab("alignment") +
    xlab("partner #") +
    ylim(0, 1) +
    scale_x_continuous(breaks = c(1,2,3)) +
    scale_color_manual(values = c('#000000', RColorBrewer::brewer.pal(5,'Blues')[5])) +
    annotate(geom="text", x=1, hjust = 0, y=.9, label="within dyads", color="#08519C") +
    annotate(geom="text", x=1, hjust = 0, y=.4, label="across dyads", color="black") +
    guides(color = FALSE) +
    theme(aspect.ratio = 4/5)

#ggsave('grid_search.pdf', height = 10, width = 10, unit = 'in')
gridExtra::grid.arrange(model.plt1, model.plt2, model.plt3, ncol = 3)
```

We investigate the qualitative predictions of this model under three increasingly complex scenarios. 
In all of these scenarios, speaker and listener agents play a reference game with a set of two objects $\{o_1, o_2\}$.
On each trial, one of these objects is designated for the speaker as the *target*. 
They must select from a set of utterance $\{u_0, \dots, u_j\}$ to convey the identity of the target to the listener.
Upon hearing this utterance, the listener selects which of the objects they believe to be the target and then receives feedback about the true target.
The resulting data $D_k$ from an interaction with partner $k$ thus consists of utterance-object pairs $\{(u, o)_t\}$ for each trial $t$.

Given this reference game setting, we can now explicitly specify the likelihood and prior terms. 
We consider a likelihood given by the Rational Speech Act (RSA) framework, which formalizes the Gricean assumption of cooperativity [@GoodmanFrank16_RSATiCS;@FrankeJager16_ProbabilisticPragmatics].
A pragmatic speaker $S_1$ attempts to trade off informativity against the cost of producing an utterance, while a pragmatic listener $L_1$ inverts their model of the speaker to infer the intended target.
The chain of recursive social reasoning grounds out in a \emph{literal listener} $L_0$, who identifies an intended meaning from their content-addressible memory for lexical items $\mathcal{L}_{\phi_k}$. 
This model can be formally specified as follows:
$$
\begin{array}{rcl}
L_0(o | u, \phi_k) &\propto  & \exp\{\mathcal{L}_{\phi_k}(u,o)\} \\
S_1(u | o, \phi_k) &\propto &  \exp\{w_I \cdot \log L_0(o | u, \phi_k) - w_C \cdot \textrm{cost}(u)\}   \\
L_1(o | u, \phi_k) &\propto  & S_1(u | o, \phi_k) P(o) 
\end{array}
$$
where $w_I$ and $w_C$ are free parameters controlling the relative weights on the informativity and cost term, respectively. 
We define $P(D_k | \phi_k)$ as the probability of the data under a pragmatic listener $L_1$.
We also use this RSA model to simulate the behavior of uncertain speakers $S$ and listeners $L$. 
Utterances and object selections are sampled from the posterior predictive, marginalizing over lexical uncertainty.

Finally, we must specify the form of the lexical prior and a method to perform inference in this model.
We assume $\Theta$ is a matrix with an entry for each utterance-object pair $(u_i, o_j)$, and use independent Gaussian distributions for each $\Theta_{ij} \in \Theta$ as a hyper-prior.
We then centered our partner-specific prior $\phi_{ij} \in \phi$ at the shared value for a particular partner:
$$\begin{array}{rcl}
P(\Theta_{ij}) & \sim & \mathcal{N}(0, 1)\\
P(\phi_{ij} | \Theta_{ij}) & \sim & \mathcal{N}(\Theta_{ij}, 1)
\end{array}$$
Note that the variances chosen in these priors represent assumptions about how far partner-specific priors can drift from the community-wide value.

For all simulations, we used variational inference [VI; @RanganathGerrishBlei13_BlackBoxVariationalInference] to perform inference. 
Variational methods transform probabilistic inference problems into optimization problems by approximating the true posterior with a parameterized family.
Specifically, we make a \emph{mean-field} approximation and assume that the full posterior can be factored into independent Gaussians for each random variable in the lexicon.
We then optimize the parameters of these posterior Gaussians by minimizing the evidence lower bound (ELBO) objective [see @murphy2012machine for more details].
Variational inference allows us to amortize inference as additional data is observed. 
We run 50,000 steps of gradient descent on the first observation to obtain a posterior, compute the agent's marginal prediction for the next observation by taking the expectation over 50,000 samples from the posterior predictive, then continue running gradient descent on the same parameters after adding the new observation in the data.

### Simulation 1: Listener accuracy across partners

The key predictions of our model concern the pattern of generalization across partners.
In our first simulation, we consider the partner-specificity of a listener's expectations about which object is being referred to.
To observe the model's behavior in the simplest case, we assume the speaker has a vocabulary of two utterances $\{u_1, u_2\}$ and feed the model the same utterance-object pair ($\{o_1, u_1\}$) on every trial.
Instead of presenting this stream of data from a single partner, or randomly choosing a different partner on every trial, we swap in a new partner every block of 4 trials.

Our simulation results are shown in Fig. \ref{fig:simulations}A.
The listener begins at chance due to its uninformative prior, but after observing several trial of evidence from the same partner, it learns to choose the true target with high accuracy.
When a new partner is introduced, it reverts nearly to its original state.
Because of the hierarchical structure of its lexical expectations, it was ambiguous whether the evidence from the first partner was idiosyncratic or due to shared structure.
After adapting to multiple partners, however, we find that it has stronger initial expectations by its fourth partner. 
Thus, expectations about what an individual partner means have gradually been incorporated into community-level expectations.

```{r task_display, fig.env = "figure*", fig.pos = "t!", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "\\label{fig:task1_display} Experimental design. (A) Participants were placed in fully-connected networks of 4 and (B) played repeated reference games with each partner."}
knitr::include_graphics("figs/design.pdf")
```


### Simulation 2: Speaker utterance length across partners

Next, we show that our model accounts for the \emph{partner-specificity} of the speaker's referring expressions [@wilkes-gibbs_coordinating_1992]. 
We supplement the utterance space with multi-word utterances built from a set of four primitives: $\{u_1, u_2, u_3, u_4\}$.
Under our model, speakers revert back to a longer description with a novel partner because evidence from a single listener is relatively uninformative about the community-level prior.
After interacting with enough partners in a tight-knit community, speakers should become increasingly confident that labels are not simply idiosyncratic features of a particular partner's lexicon but are shared across the entire community.
In other words, the partner-specific expectations agents form within an interaction to solve novel communication problems gradually generalize to community-wide expectations as they gain additional evidence of the latent population-level distribution from which different partners are sampled.
These expectations manifest in an increasing willingness to use shorter labels with novel partners (Fig. \ref{fig:simulations}B).

### Simulation 3: Network convergence

The first two simulations presented a single agent with a fixed sequence of data to understand its gradient of generalization within and across partners. 
Here, we test the consequences of the proposed hierarchical inference scheme for a network of interacting agents.
How does the network as a whole coordinate?
Do agents come to share a similar $\Theta$, suggesting that community-wide conventions have formed?
We used a round-robin scheme to schedule four agents into three blocks of interaction, with agents taking turns in the speaker and listener roles. 
From each individual agent's perspective, this experiment is identical to the earlier simulation (i.e. a series of 3 partners).
Because all agents are not learning from the others, however, the network as a whole faces a coordination problem.
For example, in the first block, agent 1 and 2 may coordinate on using $u_1$ while agent 3 and 4 coordinate no using $u_2$. 
Once they swap partners, they must re-negotiate this potential miscoordination. 
We find that the network as a whole gradually aligns (Fig. \ref{fig:simulations}C).

# Behavioral experiment: \ Convention formation on a network

To evaluate these qualitative predictions, we designed a communication experiment on a small network.
Rather than anonymizing partners, we divided the experiment into blocks of contiguous interaction with stable partners [see @fay_interactive_2010; @garrod_conversation_1994 for similar designs].
Each block was a full repeated reference game, where participants had to coordinate on an *ad hoc* convention, or *pact*, for how to refer to reoccuring target objects with their partner [@BrennanClark96_ConceptualPactsConversation].

While it has been frequently observed that messages reduce in length across repetitions as common ground is built a single partner [@krauss_changes_1964], and sharply revert back to longer utterances when a new partner is introduced [@wilkes-gibbs_coordinating_1992], the key empirical predictions distinguishing our model from alternatives concern behavior across partner boundaries.
Complete-pooling accounts predict no change in the number of words when a new partner is introduced and are thus inconsistent even with the results of @wilkes-gibbs_coordinating_1992. 
No-pooling accounts predict that roughly the same initial description length will re-occur with every subsequent interlocutor. 
Contrary to either of these extremes, our hierarchical Bayesian model predicts that description length will increase at partner boundaries but that the initial length will decrease incrementally over successive interactions: after each partner, agents should be more willing to transfer expectations from one partner to another in their community. 

## Methods

### Participants 

We recruited 92 participants from Amazon Mechanical Turk to play an interactive, natural-language reference game implemented with the Dallinger platform\footnote{http://docs.dallinger.io/}.

### Stimuli and procedure

Participants were randomly assigned to one of `r numNetworks` fully-connected four-person communities of `neighbors' (Fig. \ref{fig:task1_display}A) and paired with each of their three neighbors in a series of real-time, natural-language reference games.
Pairings were determined by a round-robin schedule (Fig. \ref{fig:task1_display}B).
Each network was randomly assigned one of three distinct sets of four abstract tangram stimuli taken from Clark and Wilkes-Gibbs [-@clark_referring_1986, see Fig. \ref{fig:task1_display}C].
These stimuli were chosen because participants do not already have strong pre-existing lexical conventions for how to refer to them (unlike photographs of common objects), but they are structured enough to support many possible descriptions (unlike images of white noise).

On each trial, one of these four shapes was highlighted as the \emph{target object} for the "speaker" who was instructed to use a chatbox to communicate the identity of this object to their partner, the "listener".
The listener could reply freely through the chatbox but was asked to ultimately make a selection from the array. 
Finally, both participants in a pair were given full feedback on each trial about their partner's choice and received bonus payment for each correct response. 

The trial sequence for a given partner was constructed so that each of the four targets appeared once per block, for four continuous blocks.
After completing sixteen trials with one partner, participants were introduced to their next partner and asked to play the repeated reference game again with the same four objects.
This process repeated until each participant had partnered with all three neighbors.
Participants in a network were assigned distinct avatars to emphasize that participants were speaking to the same partner for an extended period.
Because some pairs within the network took longer than others to complete the trial sequence, we sent participants to a temporary waiting room if their next partner was not yet ready. 
```{r reduction, cache=T, fig.env = "figure*", fig.pos = "t!", fig.width=7, fig.height=4, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:results}(A)Increase in accuracy across partners, (B) reduction in number of words across partners, (C) network convergence."}
clicks.mean <- relevantClicks %>%  
  group_by(repnum, partnernum) %>%
  tidyboot::tidyboot_mean(correct)

results.plt1 <- clicks.mean %>%
  mutate(t = repnum + 4 * partnernum) %>%
  ggplot(aes(x = t, y = empirical_stat, fill = factor(partnernum), color = factor(partnernum), group = partnernum)) +
    geom_line() +
    geom_point() +
    scale_color_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    scale_fill_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.2, color =NA) +
    scale_x_continuous(breaks = c(1,3, 5,7,9,11)) +
    theme_few() +
    theme(aspect.ratio = 1) +
    guides(color = FALSE, fill = FALSE) +
    labs(x = 'time', y = 'accuracy', tag = 'A')

messages.mean <- relevantMessages %>%
  group_by(partnernum, repnum) %>%
  tidyboot::tidyboot_mean(column = m)

results.plt2 <- messages.mean %>%
  mutate(t = repnum + 4 * partnernum) %>%
  ggplot(aes(x = t, y = empirical_stat, fill = factor(partnernum), color = factor(partnernum), group = partnernum)) +
    geom_point() +
    geom_line() +
    scale_color_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    scale_fill_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    scale_x_continuous(breaks = c(1,3, 5,7,9,11)) +
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = .2, color =NA) +
    theme_few() +
    theme(aspect.ratio = 1) +
    guides(color = FALSE, fill = FALSE) +
    ylim(0,NA) +
    labs(x = "time", y = "mean # words", tag = 'B')

partnerLookup <- messages %>%
  group_by(networkid) %>%
  do(., mutate(., speakerID = group_indices(., participantid))) %>%
  group_by(networkid, roomid, partnernum, speakerID) %>%
  tally() %>%
  group_by(networkid, roomid) %>%
  mutate(partnerID = ifelse(speakerID == first(speakerID), last(speakerID), first(speakerID))) %>%
  ungroup() %>%
  mutate(speaker1 = as.factor(speakerID)) %>%
  select(-n, -roomid) 

stopwords_regex = paste(c('ive', tm::stopwords('SMART')), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')

matches <- messages %>% 
  filter(role == "speaker") %>%
  filter(networkid %in% completeNetworks) %>%
  group_by(participantid, partnernum) %>%
  mutate(ordinalrep = ifelse(repid == min(repid), 'first', 'second'),
         content = tolower(content),
         content = stringr::str_replace_all(content, stopwords_regex, ""),
         content = str_squish(gsub("[[:punct:]]", "", content))) %>%
  group_by(networkid) %>%
  do(., mutate(., speakerID = as.integer(group_indices(., participantid)))) %>%
  group_by(ordinalrep, networkid, partnernum, stimsetid, repid, target, participantid, speakerID, trialnum) %>%
  summarize(content = paste0(content, collapse = ' ')) %>%
  group_by(networkid, repid, target, partnernum, speakerID) %>%
  tidybayes::gather_pairs(speakerID, content, row = 'speaker1', col = 'speaker2', x = 'utt1', y = 'utt2') %>%
  left_join(partnerLookup) %>%
  rowwise() %>%
  mutate(partnerType = ifelse(speaker2 == partnerID, 'within', 'across'),
         matchRate = length(intersect(strsplit(utt1, " ")[[1]], strsplit(utt2, " ")[[1]])),
         utt1Length = length(strsplit(utt1, " ")[[1]]),
         utt2Length = length(strsplit(utt2, " ")[[1]])) %>%
  arrange(networkid, partnernum)

matches.summary <- matches %>% 
  group_by(partnernum, partnerType) %>%
  tidyboot_mean(matchRate> 0)

results.plt3 <- matches.summary %>%
  ungroup() %>%
  mutate(comparison = ifelse(partnerType == 'within', 'within dyad', 'across dyads')) %>%
  ggplot(aes(x = partnernum, y = empirical_stat, color = comparison, group = comparison)) +
    geom_line() +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    labs(x = "partner #", y = "alignment", tag = 'C') +
    ggthemes::theme_few() +
    ylim(0, 1) +
    scale_color_colorblind() +
    scale_x_continuous(breaks = c(1,2,3)) +
    annotate(geom="text", x=0.5, hjust = 0, y=.9, label="within dyads", color="orange") +
    annotate(geom="text", x=0.5, hjust = 0, y=.2, label="across dyads", color="black") +
    guides(color = FALSE) +
    theme(aspect.ratio = 1) 

gridExtra::grid.arrange(results.plt1, results.plt2, results.plt3, ncol = 3)
```

## Results

We evaluate our model's predictions on the same three metrics we reported in the simulations: accuracy, utterance length, and network convergence.

### Listener accuracy

TODO. (see Fig. \ref{fig:results}A)

### Speaker utterance length

The mean number of words used per description is a standard measure of coding efficiency in reference games. 
We tested predictions using a mixed-effects regression of partner number and repetition block number on the length of the speaker's description.
We included a random-effect structure including item-effects at the object and speaker level.
\aeg{by-item, by-subject slopes and intercepts for partner \# and repetition \#?}
We find a positive jump in description length across partner-boundaries overall, t(91) = 3.7, p < 0.001, indicating sensitivity to different partners, but a successive incremental decrease in the lengths of these initial descriptions, t(79.2) = -6.8, p < 0.001 (Fig. \ref{fig:results}B).
\aeg{why t-test and not intercepts?}

### Network convergence 

TODO. (Fig. \ref{fig:results}C).

# Discussion

In other words, we suggest that conventional meanings result from agents solving a meta-learning problem, adapting to each partner along the way.

1. Other advantages of hierarchical model. e.g. it's more robust to deviations than complete-pooling; if we have a lot of interactions with idiosyncratic speakers (e.g. children), we don't replace our conventional community-level expectations. But agent-based models with a memory window or single representation predict this. Also there are other kinds of partner-specific information that may need to be tracked (e.g. visual access/knowledge, e.g. if you know someone is an expert in an area). 

2. Possible connection to memory mechanisms: partners as contexts that get reinstated [@Brown-schmidt; @horton_revisiting_2016], and \aeg{I'm not wild about "looking up" meaning in a "parameterized lexicon", preferring "a literal listener, L0, who identifies an intended meaning from their content-addressible memory which may include information about individual speakers or subgroups of speakers as well as phonological, contextual, and interpretative information" but for current purposes, fine!}

3. Suggest ideas about different communities and code-switching as targets of future work. e.g. The current work captures and quantifies  incremental convergence within communities of 4 unfamiliar English speakers towards a set of shared language conventions.  We recognize that real-world communities are more complex than this, however, as each speaker takes part in a number of subcommunities which vary in size and overlap. For example, we use largely overlapping but partially distinct conventions depending on whether we are communicating with psychologists, friends from high school, bilinguals, or children, and we comprehend certain conventions that we do not use ourselves. To model the full scale of an individual's network of communities, social factors and cues are required. A strength of the hierachical Bayesian framework is that knowledge about these communities can be learned and represented in the generative model of a speaker [@Gershman]. The current work suggests that hierarchical generalization may be a foundational cognitive building block for establishing conventionality at the group level while maintaining flexibility within interactions.

<!-- # Acknowledgements -->

<!-- Place acknowledgments (including funding information) in a section at -->
<!-- the end of the paper. -->

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
